# =============================================================================
# Over view : create the local LLM environment with RAG
# Procedure :
#   1. Define the model object with predifined model
#   2. With defined model, create the pipe line that has text-generation task
#   3. Create the vectorDB to retrieve the specific information
# =============================================================================
#------------------------------------------------------------------------------
#   1. Define the model object with predifined model
#------------------------------------------------------------------------------
# import packages
import sys
from torch import cuda, bfloat16
import torch
import transformers
from transformers import AutoTokenizer
from time import time
from langchain.llms import HuggingFacePipeline
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.vectorstores import Chroma

# set the model name downloaded from hugginface repo
model_id = "elyza/Llama-3-ELYZA-JP-8B"

# configure model with transformers
model_config = transformers.AutoConfig.from_pretrained(  
   pretrained_model_name_or_path=model_id,
   trust_remote_code=True,
   max_new_tokens=1024
)
model = transformers.AutoModelForCausalLM.from_pretrained(  
    pretrained_model_name_or_path=model_id,
    trust_remote_code=True,
    config=model_config,
    device_map='auto',
)
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)

#------------------------------------------------------------------------------
#   2. With defined model, create the pipe line that has text-generation task
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#   3. Create the vectorDB to retrieve the specific information
#------------------------------------------------------------------------------
query_pipeline = transformers.pipeline(
        task="text-generation",
        model=model,
        tokenizer=tokenizer,
        torch_dtype=torch.float16,
        max_length=1024,
        device_map="auto",
)

llm = HuggingFacePipeline(pipeline=query_pipeline)

loader = PyPDFLoader("~/ナマケモノの生態.pdf")
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100
)

all_splits = text_splitter.split_documents(documents)

model_name = "intfloat/multilingual-e5-large"
embeddings = HuggingFaceEmbeddings(model_name=model_name)

vectordb = Chroma.from_documents(
        documents=all_splits,
        embedding=embeddings,
        persist_directory="chroma_db"
)
retriever = vectordb.as_retriever()

qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    verbose=True
)

def test_rag(qa, query):
    time_start = time()
    response = qa.run(query)
    time_end = time()
    total_time = f"{round(time_end-time_start, 3)} sec."
    
    full_response =  f"Question: {query}\nAnswer: {response}\nTotal time: {total_time}"
    print(full_response)

query = "最近暑い日が続きますが、ナマケモノは元気にしてますか？"
test_rag(qa, query)
